{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../processed_data/OpenSubtitles_processed_en.txt ../processed_data/CCMatrix_processed_en.txt > ../processed_data/processed_en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../processed_data/OpenSubtitles_processed_ar.txt ../processed_data/CCMatrix_processed_ar.txt > ../processed_data/processed_ar.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train english tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \n",
      "TrainerSpec {\n",
      "  input: ../processed_data/processed_en.txt\n",
      "  input_format: \n",
      "  model_prefix: en_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "NormalizerSpec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "\n",
      "trainer_interface.cc(267) LOG(INFO) Loading corpus: ../processed_data/processed_en.txt\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(114) LOG(WARNING) Too many sentences are loaded! (2999736), which may slow down training.\n",
      "trainer_interface.cc(116) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(119) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(315) LOG(INFO) Loaded all 2999736 sentences\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(335) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(369) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(369) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(369) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(369) LOG(INFO) Found null character. The corpus must be encoded in utf-8.\n",
      "trainer_interface.cc(384) LOG(INFO) all chars count=233472515\n",
      "trainer_interface.cc(392) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(402) LOG(INFO) Alphabet size=883\n",
      "trainer_interface.cc(403) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(435) LOG(INFO) Done! preprocessed 2999736 sentences.\n",
      "unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(184) LOG(INFO) Initialized 534119 seed sentencepieces\n",
      "trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 2999736\n",
      "trainer_interface.cc(451) LOG(INFO) Done! 320217\n",
      "unigram_model_trainer.cc(470) LOG(INFO) Using 320217 sentences for EM training\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=159326 obj=9.48684 num_tokens=654889 num_tokens/piece=4.11037\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=131566 obj=7.39622 num_tokens=645721 num_tokens/piece=4.90796\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=98671 obj=7.34818 num_tokens=668977 num_tokens/piece=6.77987\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=98592 obj=7.34551 num_tokens=669052 num_tokens/piece=6.78607\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=73944 obj=7.35779 num_tokens=710808 num_tokens/piece=9.61279\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=73944 obj=7.35539 num_tokens=710739 num_tokens/piece=9.61185\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=55458 obj=7.37589 num_tokens=756073 num_tokens/piece=13.6333\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=55458 obj=7.37234 num_tokens=755996 num_tokens/piece=13.6319\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=41593 obj=7.40258 num_tokens=802313 num_tokens/piece=19.2896\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=41593 obj=7.39738 num_tokens=802241 num_tokens/piece=19.2879\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=31194 obj=7.4405 num_tokens=849911 num_tokens/piece=27.246\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=31194 obj=7.43314 num_tokens=849799 num_tokens/piece=27.2424\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=23395 obj=7.49279 num_tokens=899737 num_tokens/piece=38.4585\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=23395 obj=7.48244 num_tokens=899748 num_tokens/piece=38.459\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=17546 obj=7.56454 num_tokens=954610 num_tokens/piece=54.4061\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=17546 obj=7.5496 num_tokens=954588 num_tokens/piece=54.4049\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=13159 obj=7.66329 num_tokens=1013625 num_tokens/piece=77.029\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=13159 obj=7.64177 num_tokens=1013746 num_tokens/piece=77.0382\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=9869 obj=7.7993 num_tokens=1083051 num_tokens/piece=109.743\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=9869 obj=7.76765 num_tokens=1083182 num_tokens/piece=109.756\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=8800 obj=7.84075 num_tokens=1110410 num_tokens/piece=126.183\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=8800 obj=7.82701 num_tokens=1110570 num_tokens/piece=126.201\n",
      "trainer_interface.cc(507) LOG(INFO) Saving model: en_tokenizer.model\n",
      "trainer_interface.cc(531) LOG(INFO) Saving vocabs: en_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "! spm_train --input=../processed_data/processed_en.txt --model_prefix=en_tokenizer --vocab_size=8000 --character_coverage=1.0 --model_type=unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test english tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁the ▁weather ▁is ▁the ▁last ▁truly ▁wild ▁thing ▁on ▁earth\n"
     ]
    }
   ],
   "source": [
    "! echo \"the weather is the last truly wild thing on earth\" | spm_encode --model=en_tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2593 11 3 190 1609 3495 258 20 987\n"
     ]
    }
   ],
   "source": [
    "! echo \"the weather is the last truly wild thing on earth\" | spm_encode --model=en_tokenizer.model --output_format=id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the weather is the last truly wild thing on earth\n"
     ]
    }
   ],
   "source": [
    "! echo \"the weather is the last truly wild thing on earth\" | spm_encode --model=en_tokenizer.model --output_format=id | spm_decode --model=en_tokenizer.model --input_format=id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! spm_export_vocab --model=en_tokenizer.model --output=en_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train arabic tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \n",
      "TrainerSpec {\n",
      "  input: ../processed_data/processed_ar.txt\n",
      "  input_format: \n",
      "  model_prefix: ar_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "NormalizerSpec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "\n",
      "trainer_interface.cc(267) LOG(INFO) Loading corpus: ../processed_data/processed_ar.txt\n",
      "trainer_interface.cc(139) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(114) LOG(WARNING) Too many sentences are loaded! (1997345), which may slow down training.\n",
      "trainer_interface.cc(116) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(119) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(315) LOG(INFO) Loaded all 1997345 sentences\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(335) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(384) LOG(INFO) all chars count=107147229\n",
      "trainer_interface.cc(392) LOG(INFO) Done: 99.9556% characters are covered.\n",
      "trainer_interface.cc(402) LOG(INFO) Alphabet size=87\n",
      "trainer_interface.cc(403) LOG(INFO) Final character coverage=0.999556\n",
      "trainer_interface.cc(435) LOG(INFO) Done! preprocessed 1997343 sentences.\n",
      "unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(184) LOG(INFO) Initialized 706767 seed sentencepieces\n",
      "trainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 1997343\n",
      "trainer_interface.cc(451) LOG(INFO) Done! 663502\n",
      "unigram_model_trainer.cc(470) LOG(INFO) Using 663502 sentences for EM training\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=323240 obj=11.5349 num_tokens=1445413 num_tokens/piece=4.47164\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=277302 obj=9.62116 num_tokens=1444390 num_tokens/piece=5.20873\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=207948 obj=9.56649 num_tokens=1496086 num_tokens/piece=7.19452\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=207802 obj=9.55675 num_tokens=1497365 num_tokens/piece=7.20573\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=155851 obj=9.59094 num_tokens=1572048 num_tokens/piece=10.0869\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=155848 obj=9.58279 num_tokens=1572108 num_tokens/piece=10.0874\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=116886 obj=9.63316 num_tokens=1648990 num_tokens/piece=14.1077\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=116886 obj=9.62105 num_tokens=1649088 num_tokens/piece=14.1085\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=87664 obj=9.69253 num_tokens=1724864 num_tokens/piece=19.6759\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=87664 obj=9.677 num_tokens=1724969 num_tokens/piece=19.6771\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=65748 obj=9.76712 num_tokens=1800626 num_tokens/piece=27.3868\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=65748 obj=9.74717 num_tokens=1801140 num_tokens/piece=27.3946\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=49311 obj=9.86131 num_tokens=1876818 num_tokens/piece=38.0608\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=49311 obj=9.83628 num_tokens=1877017 num_tokens/piece=38.0649\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=36983 obj=9.97675 num_tokens=1951451 num_tokens/piece=52.7662\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=36983 obj=9.94579 num_tokens=1952312 num_tokens/piece=52.7894\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=27737 obj=10.1166 num_tokens=2027464 num_tokens/piece=73.096\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=27737 obj=10.0791 num_tokens=2027797 num_tokens/piece=73.108\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=20802 obj=10.2831 num_tokens=2105413 num_tokens/piece=101.212\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=20802 obj=10.2379 num_tokens=2105676 num_tokens/piece=101.225\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=15601 obj=10.4756 num_tokens=2186268 num_tokens/piece=140.136\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=15601 obj=10.4218 num_tokens=2186507 num_tokens/piece=140.152\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=11700 obj=10.6965 num_tokens=2267757 num_tokens/piece=193.825\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=11700 obj=10.6337 num_tokens=2268628 num_tokens/piece=193.9\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=8800 obj=10.9432 num_tokens=2354612 num_tokens/piece=267.57\n",
      "unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=8800 obj=10.8711 num_tokens=2354761 num_tokens/piece=267.586\n",
      "trainer_interface.cc(507) LOG(INFO) Saving model: ar_tokenizer.model\n",
      "trainer_interface.cc(531) LOG(INFO) Saving vocabs: ar_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "! spm_train --input=../processed_data/processed_ar.txt --model_prefix=ar_tokenizer --vocab_size=8000 --character_coverage=0.9995 --model_type=unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test arabic tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁الطقس ▁آخر ▁شيء ▁بر ّ ي ▁حقا ً ▁على ▁الأرض ِ\n"
     ]
    }
   ],
   "source": [
    "! echo \"الطقس آخر شيء برّي حقاً على الأرضِ\" | spm_encode --model=ar_tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4945 177 93 1418 38 13 296 9 12 564 67\n"
     ]
    }
   ],
   "source": [
    "! echo \"الطقس آخر شيء برّي حقاً على الأرضِ\" | spm_encode --model=ar_tokenizer.model --output_format=id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الطقس آخر شيء برّي حقاً على الأرضِ\n"
     ]
    }
   ],
   "source": [
    "! echo \"الطقس آخر شيء برّي حقاً على الأرضِ\" | spm_encode --model=ar_tokenizer.model --output_format=id | spm_decode --model=ar_tokenizer.model --input_format=id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! spm_export_vocab --model=ar_tokenizer.model --output=ar_vocab"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39c42a58bc70e49ee35f5d1a736962339f1b141129455ae363fbc37fbb042dbf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
